# -*- coding: utf-8 -*-
"""Copy_of_fakenews_AI_checkpoint.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XqZhEypVsaexEJkn3U5JLEb17ddQYE8y
"""

# Commented out IPython magic to ensure Python compatibility.
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.metrics import confusion_matrix
# %matplotlib inline
import seaborn as sns
import numpy as np # linear algebra
import pandas as pd #data processing
import os
import re
import nltk #statastical natural lang

from google.colab import drive
drive.mount('/content/drive')

train=pd.read_csv('/content/drive/MyDrive/ML/FNP/train.csv')
test=pd.read_csv('/content/drive/MyDrive/ML/FNP/test.csv')

test.head()

train.head()

train=train.drop(['id','author'], axis=1)

test = test.drop(['id','author'], axis=1)

test.head()

train.head()

print(train.shape, test.shape)

print(train.isnull().sum())
print('************')
print(test.isnull().sum())

test=test.fillna(' ')
train=train.fillna(' ')
test['total']=test['title']+' '+test['text']
train['total']=train['title']+' '+train['text']

real_words = ''
fake_words = ''
stopwords = set(STOPWORDS)

# iterate through the csv file
for val in train[train['label']==1].total:

    # split the value
    tokens = val.split()

    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()

    real_words += " ".join(tokens)+" "

for val in train[train['label']==0].total:

    # split the value
    tokens = val.split()

    #Convertseachtokenintolowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()

    fake_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(real_words)

#plottheWordCloudimage
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

"""# Cleaning and preprocessing

# 1. Regex
"""

#Remove punctuations from the String
s = "!</> hello please$$ </>^s!!!u%%bs&&%$cri@@@be^^^&&!& </>*to@# the&&\ cha@@@n##%^^&nel!@# %%$"

s = re.sub(r'[^\w\s]','',s)

print(s)

"""# 2. Tokenization"""

#Downloading nltk data
nltk.download('punkt')

nltk.word_tokenize("Hello how are you")

"""# 3. StopWords"""

# from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
print(stop_words)

sentence = "Covid-19 pandemic has impacted many countries and what it did to economy is very stressful"

words = nltk.word_tokenize(sentence)
words = [w for w in words if w not in stop_words]

words

"""
# 4. Lemmatization"""

# Setup
!pip install -q wordcloud
import wordcloud

import nltk
nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

import pandas as pd
import matplotlib.pyplot as plt
import io
import unicodedata
import numpy as np
import re
import string


#Tokenize the sentence and Lemmatize

from nltk.stem import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()

input_str="been had done languages cities mice"
input_str=nltk.word_tokenize(input_str)
for word in input_str:
    print(lemmatizer.lemmatize(word))

!pip install -q wordcloud
import wordcloud

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

import pandas as pd
import matplotlib.pyplot as plt
import io
import unicodedata
import numpy as np
import re
import string

from nltk.stem import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()

for index,row in train.iterrows():
    filter_sentence = ''

    sentence = row['total']
    sentence = re.sub(r'[^\w\s]','',sentence) #cleaning

    words = nltk.word_tokenize(sentence) #tokenization

    words = [w for w in words if not w in stop_words]  #stopwords removal

    for word in words:
        filter_sentence = filter_sentence + ' ' + str(lemmatizer.lemmatize(word)).lower()

    train.loc[index,'total'] = filter_sentence

train = train[['total','label']]

"""

```
# This is formatted as code
```

# Applying NLP Techniques"""

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

X_train = train['total']
Y_train = train['label']

"""# Bag-of-words / CountVectorizer"""

corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third one.',
    'Is this the first document?',
]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names())

print(X.toarray())

"""# TF-iDF Vectorizer"""

def vectorize_text(features, max_features):
    vectorizer = TfidfVectorizer( stop_words='english',
                            decode_error='strict',
                            analyzer='word',
                            ngram_range=(1, 2),
                            max_features=max_features
                            #max_df=0.5 # Verwendet im ML-Kurs unter Preprocessing
                            )
    feature_vec = vectorizer.fit_transform(features)
    return feature_vec.toarray()

tfidf_features = vectorize_text(['hello how are you doing','hi i am doing fine'],30)

tfidf_features

"""# Let's Apply"""

#Feature extraction using count vectorization and tfidf.
count_vectorizer = CountVectorizer()
count_vectorizer.fit_transform(X_train)
freq_term_matrix = count_vectorizer.transform(X_train)
tfidf = TfidfTransformer(norm="l2")
tfidf.fit(freq_term_matrix)
tf_idf_matrix = tfidf.fit_transform(freq_term_matrix)

tf_idf_matrix

"""# Modelling"""

test_counts = count_vectorizer.transform(test['total'].values)
test_tfidf = tfidf.transform(test_counts)

#split in samples
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(tf_idf_matrix, Y_train, random_state=0)

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(C=1e5)
logreg.fit(X_train, y_train)
pred = logreg.predict(X_test)
print('Accuracy of Lasso classifier on training set: {:.2f}'
     .format(logreg.score(X_train, y_train)))
print('Accuracy of Lasso classifier on test set: {:.2f}'
     .format(logreg.score(X_test, y_test)))
from sklearn.naive_bayes import MultinomialNB
cm = confusion_matrix(y_test, pred)
cm

"""# MultinomialNB"""

from sklearn.naive_bayes import MultinomialNB

NB = MultinomialNB()
NB.fit(X_train, y_train)
pred = NB.predict(X_test)
print('Accuracy of NB  classifier on training set: {:.2f}'
     .format(NB.score(X_train, y_train)))
print('Accuracy of NB classifier on test set: {:.2f}'
     .format(NB.score(X_test, y_test)))
cm = confusion_matrix(y_test, pred)
cm